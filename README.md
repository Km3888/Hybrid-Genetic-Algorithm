# Hybrid-Genetic-Algorithm

Code written for my 2018 Deans' Undergraduate Research Fellowship, for which I researched Deep Reinforcement Learning Methods. The report I've included was written at the end of the summer and is technically the conclusion of my research project. In the report I detail my proposed hybrid algorithm, which combined genetic algorithms with the so-called "vanilla" policy gradient methods and compare the results to just the genetic algorithm alone. Unsatisfied with these results, I improved my hybrid algorithm by using a more sophisticated gradient-based algorithm, known as Proximal Policy Optimization (PPO), in place of the vanilla policy gradient. By integrating my original code with the neatly modularized PPO code provided by Watcher Wang (https://github.com/watchernyu), I was able to increase the sample-efficiency of my algorithm, putting it on the same level as the plain genetic algorithm for the MuJoCo tasks tested. Ultimately, this falls short of the goal of improving upon the sample-efficiency of the genetic algorithm. Interestingly, the lack of compatability between these two algorithms has intriguing implications for their underlying mechanisms for finding optima. It shows that the two algorithms are not simply different methods for taking steps in the same directions in parameter space, but instead take steps in different (and often conflicting) directions as otherwise the addition of PPO steps would allow the genetic algorithm to converge to the same solution in fewer steps.
